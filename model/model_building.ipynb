{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31f022",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TITANIC SURVIVAL PREDICTION SYSTEM - MODEL DEVELOPMENT\n",
    "# Artificial Intelligence Course Project\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Import Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             ConfusionMatrixDisplay)\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TITANIC SURVIVAL PREDICTION SYSTEM - MODEL DEVELOPMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Load the Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 2: Loading Titanic Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the dataset\n",
    "# Download from: https://www.kaggle.com/c/titanic/data\n",
    "# Or use seaborn's built-in dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   Total passengers: {len(df)}\")\n",
    "print(f\"   Total features: {df.shape[1]}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìä First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display dataset info\n",
    "print(\"\\nüìã Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Feature Selection (Select 5 from the 7 recommended)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: Feature Selection\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Recommended features: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked\n",
    "# Target: Survived\n",
    "\n",
    "# Select 5 input features (you can change these)\n",
    "selected_features = [\n",
    "    'Pclass',      # Ticket class (1st, 2nd, 3rd)\n",
    "    'Sex',         # Gender\n",
    "    'Age',         # Age in years\n",
    "    'Fare',        # Passenger fare\n",
    "    'Embarked'     # Port of embarkation (C, Q, S)\n",
    "]\n",
    "\n",
    "target = 'Survived'\n",
    "\n",
    "# Create subset with selected features\n",
    "df_subset = df[selected_features + [target]].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Selected 5 input features from the 7 recommended:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "print(f\"\\n   Target variable: {target}\")\n",
    "\n",
    "print(f\"\\nüìä Subset shape: {df_subset.shape}\")\n",
    "\n",
    "# Target distribution\n",
    "print(\"\\nüéØ Target Variable Distribution:\")\n",
    "survived_counts = df_subset[target].value_counts().sort_index()\n",
    "print(survived_counts)\n",
    "print(f\"\\n   Did Not Survive (0): {survived_counts[0]} ({survived_counts[0]/len(df_subset)*100:.1f}%)\")\n",
    "print(f\"   Survived (1): {survived_counts[1]} ({survived_counts[1]/len(df_subset)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Data Preprocessing\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: Data Preprocessing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 4.1: Check for Missing Values\n",
    "print(\"\\nüîç Checking for missing values...\")\n",
    "missing_values = df_subset.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# 4.2: Handle Missing Values\n",
    "print(\"\\nüîß Handling missing values...\")\n",
    "\n",
    "# Age: Fill with median\n",
    "if df_subset['Age'].isnull().sum() > 0:\n",
    "    median_age = df_subset['Age'].median()\n",
    "    df_subset['Age'] = df_subset['Age'].fillna(median_age)\n",
    "    print(f\"   ‚úì Age: Filled {missing_values['Age']} missing values with median ({median_age:.1f})\")\n",
    "\n",
    "# Fare: Fill with median\n",
    "if df_subset['Fare'].isnull().sum() > 0:\n",
    "    median_fare = df_subset['Fare'].median()\n",
    "    df_subset['Fare'] = df_subset['Fare'].fillna(median_fare)\n",
    "    print(f\"   ‚úì Fare: Filled missing values with median\")\n",
    "\n",
    "# Embarked: Fill with mode\n",
    "if df_subset['Embarked'].isnull().sum() > 0:\n",
    "    mode_embarked = df_subset['Embarked'].mode()[0]\n",
    "    df_subset['Embarked'] = df_subset['Embarked'].fillna(mode_embarked)\n",
    "    print(f\"   ‚úì Embarked: Filled {missing_values['Embarked']} missing values with mode ({mode_embarked})\")\n",
    "\n",
    "print(f\"\\n‚úÖ All missing values handled. Remaining: {df_subset.isnull().sum().sum()}\")\n",
    "\n",
    "# 4.3: Encode Categorical Variables\n",
    "print(\"\\nüîÑ Encoding categorical variables...\")\n",
    "\n",
    "# Sex: Male=1, Female=0\n",
    "df_subset['Sex'] = df_subset['Sex'].map({'male': 1, 'female': 0})\n",
    "print(\"   ‚úì Sex encoded (male=1, female=0)\")\n",
    "\n",
    "# Embarked: Use Label Encoding\n",
    "le_embarked = LabelEncoder()\n",
    "df_subset['Embarked_Encoded'] = le_embarked.fit_transform(df_subset['Embarked'])\n",
    "print(f\"   ‚úì Embarked encoded into {df_subset['Embarked_Encoded'].nunique()} categories\")\n",
    "print(f\"      Mapping: {dict(zip(le_embarked.classes_, le_embarked.transform(le_embarked.classes_)))}\")\n",
    "\n",
    "# Save the encoder\n",
    "import os\n",
    "os.makedirs('model', exist_ok=True)\n",
    "joblib.dump(le_embarked, 'model/embarked_encoder.pkl')\n",
    "print(\"   ‚úì Embarked encoder saved to 'model/embarked_encoder.pkl'\")\n",
    "\n",
    "# Drop original Embarked column\n",
    "df_subset = df_subset.drop('Embarked', axis=1)\n",
    "\n",
    "# 4.4: Separate Features and Target\n",
    "X = df_subset.drop('Survived', axis=1)\n",
    "y = df_subset['Survived']\n",
    "\n",
    "print(f\"\\n‚úÖ Features (X): {X.shape}\")\n",
    "print(f\"‚úÖ Target (y): {y.shape}\")\n",
    "print(f\"\\n   Final features: {list(X.columns)}\")\n",
    "\n",
    "# Display preprocessed data\n",
    "print(\"\\nüìä Preprocessed Data (first 5 rows):\")\n",
    "print(X.head())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Exploratory Data Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: Exploratory Data Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "print(df_subset.describe())\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\nüìà Correlation with Survival:\")\n",
    "correlation = df_subset.corr()['Survived'].sort_values(ascending=False)\n",
    "print(correlation)\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions by Survival Status', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "features_to_plot = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked_Encoded']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Create survival vs non-survival distributions\n",
    "    survived = df_subset[df_subset['Survived'] == 1][feature]\n",
    "    not_survived = df_subset[df_subset['Survived'] == 0][feature]\n",
    "    \n",
    "    ax.hist([not_survived, survived], bins=20, label=['Did Not Survive', 'Survived'],\n",
    "            color=['red', 'green'], alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel(feature, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title(f'{feature} Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[1, 2].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Survival rate by feature\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Survival Rates by Key Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "# By Pclass\n",
    "pclass_survival = df_subset.groupby('Pclass')['Survived'].mean()\n",
    "axes[0].bar(pclass_survival.index, pclass_survival.values, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Passenger Class', fontweight='bold')\n",
    "axes[0].set_ylabel('Survival Rate', fontweight='bold')\n",
    "axes[0].set_title('Survival Rate by Class')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By Sex\n",
    "sex_survival = df_subset.groupby('Sex')['Survived'].mean()\n",
    "axes[1].bar(['Female', 'Male'], sex_survival.values, color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Gender', fontweight='bold')\n",
    "axes[1].set_ylabel('Survival Rate', fontweight='bold')\n",
    "axes[1].set_title('Survival Rate by Gender')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By Embarked\n",
    "embarked_survival = df_subset.groupby('Embarked_Encoded')['Survived'].mean()\n",
    "axes[2].bar(le_embarked.classes_, embarked_survival.values, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_xlabel('Port of Embarkation', fontweight='bold')\n",
    "axes[2].set_ylabel('Survival Rate', fontweight='bold')\n",
    "axes[2].set_title('Survival Rate by Embarkation Port')\n",
    "axes[2].set_ylim([0, 1])\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Train-Test Split\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: Train-Test Split\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split data: 80% train, 20% test with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data split completed (80-20 with stratification)\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Testing samples: {len(X_test)}\")\n",
    "\n",
    "print(f\"\\n   Train class distribution:\")\n",
    "train_dist = y_train.value_counts().sort_index()\n",
    "print(f\"      Did Not Survive: {train_dist[0]} ({train_dist[0]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"      Survived: {train_dist[1]} ({train_dist[1]/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Feature Scaling\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: Feature Scaling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Features scaled using StandardScaler\")\n",
    "print(\"   Note: Scaling improves model performance for distance-based algorithms\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'model/scaler.pkl')\n",
    "print(\"   ‚úì Scaler saved to 'model/scaler.pkl'\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Model Training - Random Forest Classifier\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: Model Training - Random Forest Classifier\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and train Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    max_depth=10,          # Maximum depth of trees\n",
    "    min_samples_split=5,   # Minimum samples to split\n",
    "    random_state=42,\n",
    "    n_jobs=-1              # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"\\n‚è≥ Training Random Forest Classifier...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"‚úÖ Model training completed!\")\n",
    "\n",
    "# Display model parameters\n",
    "print(\"\\nüìã Model Parameters:\")\n",
    "print(f\"   Algorithm: Random Forest Classifier\")\n",
    "print(f\"   Number of trees: {model.n_estimators}\")\n",
    "print(f\"   Max depth: {model.max_depth}\")\n",
    "print(f\"   Random state: {model.random_state}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Feature Importance:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'],\n",
    "         color='teal', edgecolor='black')\n",
    "plt.xlabel('Importance', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Feature', fontweight='bold', fontsize=12)\n",
    "plt.title('Feature Importance in Random Forest Model', fontweight='bold', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: Model Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 9: Model Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for testing set\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nüìä TRAINING SET METRICS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Accuracy  : {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   Precision : {train_prec:.4f}\")\n",
    "print(f\"   Recall    : {train_rec:.4f}\")\n",
    "print(f\"   F1-Score  : {train_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüìä TESTING SET METRICS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Accuracy  : {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Precision : {test_prec:.4f}\")\n",
    "print(f\"   Recall    : {test_rec:.4f}\")\n",
    "print(f\"   F1-Score  : {test_f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORT (Test Set):\")\n",
    "print(\"-\" * 70)\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['Did Not Survive', 'Survived']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix - Test Set\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['Did Not Survive', 'Survived'])\n",
    "disp.plot(cmap='Blues', ax=axes[0], values_format='d')\n",
    "axes[0].set_title('Confusion Matrix - Test Set', fontweight='bold', fontsize=14)\n",
    "axes[0].grid(False)\n",
    "\n",
    "# Confusion Matrix - Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized,\n",
    "                                   display_labels=['Did Not Survive', 'Survived'])\n",
    "disp_norm.plot(cmap='Greens', ax=axes[1], values_format='.2f')\n",
    "axes[1].set_title('Confusion Matrix - Normalized', fontweight='bold', fontsize=14)\n",
    "axes[1].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüí° MODEL INTERPRETATION:\")\n",
    "print(f\"   ‚Ä¢ The model correctly predicts survival with {test_acc*100:.1f}% accuracy\")\n",
    "print(f\"   ‚Ä¢ Precision of {test_prec:.2f} means {test_prec*100:.1f}% of predicted survivors actually survived\")\n",
    "print(f\"   ‚Ä¢ Recall of {test_rec:.2f} means {test_rec*100:.1f}% of actual survivors were identified\")\n",
    "print(f\"   ‚Ä¢ F1-Score of {test_f1:.2f} balances precision and recall\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: Save the Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 10: Save the Trained Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save the model using joblib\n",
    "model_filename = 'model/titanic_survival_model.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved successfully!\")\n",
    "print(f\"   Location: {model_filename}\")\n",
    "print(f\"   Method: Joblib\")\n",
    "print(f\"   File size: {os.path.getsize(model_filename) / 1024:.2f} KB\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 11: Test Model Reload\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 11: Demonstrate Model Reload Without Retraining\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîÑ Testing model reload...\")\n",
    "\n",
    "# Reload the model\n",
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_scaler = joblib.load('model/scaler.pkl')\n",
    "loaded_encoder = joblib.load('model/embarked_encoder.pkl')\n",
    "\n",
    "print(\"‚úÖ Model, scaler, and encoder reloaded successfully!\")\n",
    "\n",
    "# Make sample predictions\n",
    "print(\"\\nüìä Sample Predictions (First 5 Passengers in Test Set):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "sample_predictions = loaded_model.predict(X_test_scaled[:5])\n",
    "\n",
    "for i in range(5):\n",
    "    actual = y_test.iloc[i]\n",
    "    predicted = sample_predictions[i]\n",
    "    passenger_data = X_test.iloc[i]\n",
    "    \n",
    "    result = \"‚úì CORRECT\" if actual == predicted else \"‚úó INCORRECT\"\n",
    "    \n",
    "    print(f\"\\nPassenger {i+1}:\")\n",
    "    print(f\"   Class: {int(passenger_data['Pclass'])}, Sex: {'Male' if passenger_data['Sex']==1 else 'Female'}, Age: {passenger_data['Age']:.1f}\")\n",
    "    print(f\"   Fare: ${passenger_data['Fare']:.2f}\")\n",
    "    print(f\"   Actual: {'Survived' if actual==1 else 'Did Not Survive'}\")\n",
    "    print(f\"   Predicted: {'Survived' if predicted==1 else 'Did Not Survive'}\")\n",
    "    print(f\"   {result}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 12: Save Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 12: Save Model Configuration\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save feature names and model configuration\n",
    "model_config = {\n",
    "    'feature_names': list(X.columns),\n",
    "    'selected_features': selected_features,\n",
    "    'model_type': 'RandomForestClassifier',\n",
    "    'metrics': {\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_precision': test_prec,\n",
    "        'test_recall': test_rec,\n",
    "        'test_f1': test_f1\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_config, 'model/model_config.pkl')\n",
    "print(\"‚úÖ Model configuration saved to 'model/model_config.pkl'\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"MODEL DEVELOPMENT COMPLETED SUCCESSFULLY! ‚úÖ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüì¶ SAVED FILES:\")\n",
    "print(\"   1. model/titanic_survival_model.pkl   - Trained Random Forest model\")\n",
    "print(\"   2. model/scaler.pkl                    - Feature scaler\")\n",
    "print(\"   3. model/embarked_encoder.pkl          - Embarked port encoder\")\n",
    "print(\"   4. model/model_config.pkl              - Model configuration\")\n",
    "\n",
    "print(\"\\nüìä FINAL MODEL PERFORMANCE:\")\n",
    "print(f\"   Algorithm: Random Forest Classifier\")\n",
    "print(f\"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Precision: {test_prec:.4f}\")\n",
    "print(f\"   Recall: {test_rec:.4f}\")\n",
    "print(f\"   F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ NEXT STEPS:\")\n",
    "print(\"   1. Build web GUI (app.py + index.html)\")\n",
    "print(\"   2. Test the application locally\")\n",
    "print(\"   3. Upload to GitHub\")\n",
    "print(\"   4. Deploy to Render/PythonAnywhere/Streamlit Cloud\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
